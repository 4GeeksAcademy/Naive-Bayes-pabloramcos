{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Análisis Exploratorio de Datos (EDA)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Planteamiento del problema y recopilación de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "hello\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "import json\n",
                "import numpy as np\n",
                "from numpy._core.defchararray import upper\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "import pickle\n",
                "from sklearn.feature_selection import f_classif, SelectKBest\n",
                "\n",
                "\n",
                "\n",
                "# Leer el archivo CSV\n",
                "df = pd.read_csv('', sep=',') # Este archivo CSV contiene comas como separadores\n",
                "print(df.head())\n",
                "print(df.columns) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploración y limpieza de datos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comprobamos las dimensiones del dataframe y, además de si los datos concuerdan con el número total, vemos el tipo de dato y podemos separar entre variables categóricas y numéricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Dimensiones del dataframe: {df.shape}\")\n",
                "print(df.info())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Procedemos a contabilizar los nulos y únicos: "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Valores null por columna: \\n{df.isnull().sum()}\")\n",
                "print(f\"Valores unicos por columna: \\n{df.nunique()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Resumen de cada columna:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Eliminamos las columnas que no nos aportan datos relevantes\n",
                "\n",
                "Inicialmente comprobamos que no haya posibles duplicados en datos que generen conflictos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(df.drop(\"\", axis = 1).duplicated().sum())\n",
                "\n",
                "\n",
                "total_dataf = df.drop([''], axis=1, inplace=False)\n",
                "print(total_dataf.shape)\n",
                "print(total_dataf.columns)\n",
                "total_dataf.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Análisis de variables univariante"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generamos gráficos con los valores categóricos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axis = plt.subplots(2, 2, figsize = (14, 8))\n",
                "\n",
                "# Crear un histograma múltiple\n",
                "\n",
                "sns.countplot(ax = axis[0, 0], data = total_dataf, x = \"\")\n",
                "axis[0,0].tick_params(axis=\"x\", rotation=70)\n",
                "sns.countplot(ax = axis[0, 1], data = total_dataf, x = \"\").set(ylabel = None)\n",
                "sns.countplot(ax = axis[1, 0], data = total_dataf, x = \"\").set(ylabel = None)\n",
                "\n",
                "fig.delaxes(axis[1, 1])\n",
                "\n",
                "# Ajustar el layout\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "\n",
                "# Mostrar el plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generamos gráficos con los valores numéricos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axis = plt.subplots(4, 4, figsize = (18, 12), gridspec_kw={'height_ratios': [4, 1, 4, 1]})\n",
                "\n",
                "# Crear una figura múltiple con histogramas y diagramas de caja\n",
                "sns.histplot(ax = axis[0, 0], data = total_dataf, x = \"\").set(xlabel = None)\n",
                "sns.boxplot(ax = axis[1, 0], data = total_dataf, x = \"\")\n",
                "\n",
                "sns.histplot(ax = axis[0, 1], data = total_dataf, x = \"\").set(xlabel = None, ylabel = None)\n",
                "sns.boxplot(ax = axis[1, 1], data = total_dataf, x = \"\")\n",
                "\n",
                "\n",
                "# Ajustar el layout\n",
                "plt.tight_layout()\n",
                "\n",
                "# Mostrar el plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Análisis de variables multivariante"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Análisis numérico-numérico\n",
                "\n",
                "Tomaremos el dato \"y\" como variable objetivo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axis = plt.subplots(4, 4, figsize = (23, 8), gridspec_kw={'height_ratios': [3, 1, 3, 1]})\n",
                "\n",
                "# Crear un diagrama de dispersión múltiple\n",
                "sns.regplot(ax = axis[0, 0], data = total_dataf, x = \"\", y = \"y\")\n",
                "sns.heatmap(total_dataf[[\"y\", \"\"]].corr(), annot = True, fmt = \".2f\", ax = axis[1, 0], cbar = False)\n",
                "\n",
                "sns.regplot(ax = axis[0, 1], data = total_dataf, x = \"\", y = \"y\").set(ylabel=None)\n",
                "sns.heatmap(total_dataf[[\"y\", \"\"]].corr(), annot = True, fmt = \".2f\", ax = axis[1, 1])\n",
                "\n",
                "\n",
                "# Ajustar el layout\n",
                "plt.tight_layout()\n",
                "\n",
                "# Mostrar el plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Combinaciones entre variables numéricas\n",
                "\n",
                "Vamos a explorar la relación entre: "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axis = plt.subplots(2, 3, figsize = (20, 8), gridspec_kw={'height_ratios': [4, 1]})\n",
                "\n",
                "# Crear un diagrama de dispersión múltiple\n",
                "sns.regplot(ax = axis[0, 0], data = total_dataf, x = \"\", y = \"\")\n",
                "sns.heatmap(total_dataf[[\"\", \"\"]].corr(), annot = True, fmt = \".2f\", ax = axis[1, 0], cbar = False)\n",
                "\n",
                "\n",
                "# Ajustar el layout\n",
                "plt.tight_layout()\n",
                "\n",
                "# Mostrar el plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Combinaciones post-mapa de calor"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Análisis categórico-categórico\n",
                "\n",
                "No se puede realizar un primer análisis con la variable objetivo ya que no sería lógico factorizar \"price\" por la gran cantidad de valores que crearía. Por lo tanto, seguiremos con el análisis."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Combinaciones de la clase con varias predictoras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axis = plt.subplots(figsize = (10, 5), ncols = 1)\n",
                "\n",
                "sns.barplot(data = total_dataf, x = \"\", y = \"\", hue = \"\")\n",
                "axis.tick_params(axis=\"x\", rotation=50)\n",
                "\n",
                "plt.tight_layout()\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Análisis de correlaciones (completo)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Factorizar las variables categóricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lista_a_factorizar = [\"\"]\n",
                "\n",
                "for var in lista_a_factorizar:\n",
                "  url_transformado = \"../data/processed/\" + var + \"_transformation_rules.json\"\n",
                "  var_n = var+\"_n\"\n",
                "\n",
                "  total_dataf[var_n] = pd.factorize(total_dataf[var])[0]\n",
                "  total_dataf[[var_n, var]]\n",
                "\n",
                "  transformation_rules = {row[var]: row[var_n] for index, row in total_dataf[[var_n, var]].drop_duplicates().iterrows()}\n",
                "\n",
                "  with open(url_transformado, \"w\") as f:\n",
                "    json.dump(transformation_rules, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Mapa de calor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cols_num = [\"\"]\n",
                "fig, ax = plt.subplots(figsize=(10,7))\n",
                "sns.heatmap(total_dataf[cols_num].corr(method=\"pearson\"), annot=True, fmt=\".2f\", cmap=\"viridis\", ax=ax)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sns.pairplot(data = total_dataf)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature engineering"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Análisis de outliers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FINAL_COLS = [\"\"]\n",
                "total_dataf = total_dataf[FINAL_COLS]\n",
                "total_dataf.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Guardamos todos los datasets:\n",
                "\n",
                "total_data_CON_outliers = total_dataf.copy()\n",
                "total_data_SIN_outliers = total_dataf.copy()\n",
                "\n",
                "outliers_cols = [\"\"]\n",
                "\n",
                "def replace_outliers(column, df):\n",
                "  col_stats = total_dataf[column].describe()\n",
                "  col_iqr = col_stats[\"75%\"] - col_stats[\"25%\"]\n",
                "  upper_limit = round(float(col_stats[\"75%\"] + 1.5 * col_iqr), 2)\n",
                "  lower_limit = round(float(col_stats[\"25%\"] - 1.5 * col_iqr), 2)\n",
                "\n",
                "  if lower_limit < 0: lower_limit = min(df[column])\n",
                "  # Vamos a quitar los outliers superiores\n",
                "  df[column] = df[column].apply(lambda x: x if (x <= upper_limit) else upper_limit)\n",
                "  # Vamos a quitar los outliers inferiores\n",
                "  df[column] = df[column].apply(lambda x: x if (x >= lower_limit) else lower_limit)\n",
                "  return df.copy(), [lower_limit, upper_limit]\n",
                "\n",
                "outliers_dict = {}\n",
                "for column in outliers_cols:\n",
                "  total_data_SIN_outliers, limits = replace_outliers(column, total_data_SIN_outliers)\n",
                "  outliers_dict.update({column: limits})\n",
                "\n",
                "with open(\"../data/processed/outliers_dict.json\", \"w\") as f:\n",
                "  json.dump(outliers_dict, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Análisis de valores faltantes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(total_data_CON_outliers.isnull().sum().sort_values(ascending=False))\n",
                "total_data_SIN_outliers.isnull().sum().sort_values(ascending=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "mean(),median(),moda()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inferencia de nuevas características"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Escalado de valores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictoras = [\"\"]\n",
                "target = \"\"\n",
                "\n",
                "X_CON = total_data_CON_outliers.drop(target, axis = 1)[predictoras]\n",
                "X_SIN = total_data_SIN_outliers.drop(target, axis = 1)[predictoras]\n",
                "y = total_data_CON_outliers[target]\n",
                "\n",
                "X_train_CON_outliers, X_test_CON_outliers, y_train, y_test = train_test_split(X_CON, y, test_size = 0.2, random_state = 10)\n",
                "X_train_SIN_outliers, X_test_SIN_outliers = train_test_split(X_SIN, test_size = 0.2, random_state = 10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalización\n",
                "\n",
                "norm_CON_outliers = StandardScaler()\n",
                "\n",
                "norm_CON_outliers.fit(X_train_CON_outliers)\n",
                "\n",
                "X_train_CON_outliers_norm = norm_CON_outliers.transform(X_train_CON_outliers)\n",
                "X_train_CON_outliers_norm = pd.DataFrame(X_train_CON_outliers_norm, index = X_train_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_CON_outliers_norm = norm_CON_outliers.transform(X_test_CON_outliers)\n",
                "X_test_CON_outliers_norm = pd.DataFrame(X_test_CON_outliers_norm, index = X_test_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "# SIN OUTLIERS\n",
                "norm_SIN_outliers = StandardScaler()\n",
                "norm_SIN_outliers.fit(X_train_SIN_outliers)\n",
                "\n",
                "X_train_SIN_outliers_norm = norm_SIN_outliers.transform(X_train_SIN_outliers)\n",
                "X_train_SIN_outliers_norm = pd.DataFrame(X_train_SIN_outliers_norm, index = X_train_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_SIN_outliers_norm = norm_SIN_outliers.transform(X_test_SIN_outliers)\n",
                "X_test_SIN_outliers_norm = pd.DataFrame(X_test_SIN_outliers_norm, index = X_test_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "\n",
                "# ESCALADO MIN-MAX (MINMAXIMIZACIÓN)\n",
                "\n",
                "scaler_CON_outliers = MinMaxScaler()\n",
                "scaler_CON_outliers.fit(X_train_CON_outliers)\n",
                "\n",
                "X_train_CON_outliers_scal = scaler_CON_outliers.transform(X_train_CON_outliers)\n",
                "X_train_CON_outliers_scal = pd.DataFrame(X_train_CON_outliers_scal, index = X_train_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_CON_outliers_scal = scaler_CON_outliers.transform(X_test_CON_outliers)\n",
                "X_test_CON_outliers_scal = pd.DataFrame(X_test_CON_outliers_scal, index = X_test_CON_outliers.index, columns = predictoras)\n",
                "\n",
                "# SIN OUTLIERS\n",
                "scaler_SIN_outliers = MinMaxScaler()\n",
                "scaler_SIN_outliers.fit(X_train_SIN_outliers)\n",
                "\n",
                "X_train_SIN_outliers_scal = scaler_SIN_outliers.transform(X_train_SIN_outliers)\n",
                "X_train_SIN_outliers_scal = pd.DataFrame(X_train_SIN_outliers_scal, index = X_train_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "X_test_SIN_outliers_scal = scaler_SIN_outliers.transform(X_test_SIN_outliers)\n",
                "X_test_SIN_outliers_scal = pd.DataFrame(X_test_SIN_outliers_scal, index = X_test_SIN_outliers.index, columns = predictoras)\n",
                "\n",
                "\n",
                "# Guardado de los datasets resultantes\n",
                "X_train_CON_outliers.to_excel(\"../data/processed/X_train_CON_outliers.xlsx\", index = False)\n",
                "X_train_CON_outliers_norm.to_excel(\"../data/processed/X_train_CON_outliers_norm.xlsx\", index = False)\n",
                "X_train_CON_outliers_scal.to_excel(\"../data/processed/X_train_CON_outliers_scal.xlsx\", index = False)\n",
                "X_train_SIN_outliers.to_excel(\"../data/processed/X_train_SIN_outliers.xlsx\", index = False)\n",
                "X_train_SIN_outliers_norm.to_excel(\"../data/processed/X_train_SIN_outliers_norm.xlsx\", index = False)\n",
                "X_train_SIN_outliers_scal.to_excel(\"../data/processed/X_train_SIN_outliers_scal.xlsx\", index = False)\n",
                "\n",
                "X_test_CON_outliers.to_excel(\"../data/processed/X_test_CON_outliers.xlsx\", index = False)\n",
                "X_test_CON_outliers_norm.to_excel(\"../data/processed/X_test_CON_outliers_norm.xlsx\", index = False)\n",
                "X_test_CON_outliers_scal.to_excel(\"../data/processed/X_test_CON_outliers_scal.xlsx\", index = False)\n",
                "X_test_SIN_outliers.to_excel(\"../data/processed/X_test_SIN_outliers.xlsx\", index = False)\n",
                "X_test_SIN_outliers_norm.to_excel(\"../data/processed/X_test_SIN_outliers_norm.xlsx\", index = False)\n",
                "X_test_SIN_outliers_scal.to_excel(\"../data/processed/X_test_SIN_outliers_scal.xlsx\", index = False)\n",
                "\n",
                "y_train.to_excel(\"../data/processed/y_train.xlsx\", index = False)\n",
                "y_test.to_excel(\"../data/processed/y_test.xlsx\", index = False)\n",
                "\n",
                "# SCALERS\n",
                "\n",
                "with open(\"../models/norm_CON_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(norm_CON_outliers, file)\n",
                "with open(\"../models/norm_SIN_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(norm_SIN_outliers, file)\n",
                "with open(\"../models/scaler_CON_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(scaler_CON_outliers, file)\n",
                "with open(\"../models/scaler_SIN_outliers.pkl\", \"wb\") as file:\n",
                "  pickle.dump(scaler_SIN_outliers, file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = X_train_CON_outliers.copy()\n",
                "X_test = X_test_CON_outliers.copy()\n",
                "\n",
                "selection_model = SelectKBest(f_classif, k = 5)\n",
                "selection_model.fit(X_train, y_train)\n",
                "\n",
                "ix = selection_model.get_support()\n",
                "X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns = X_train.columns.values[ix])\n",
                "X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns = X_test.columns.values[ix])\n",
                "\n",
                "X_train_sel.head()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
